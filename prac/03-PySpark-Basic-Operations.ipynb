{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **PySpark Basic Operations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/13 14:34:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .appName(\"My First pyspark app\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Load Files**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Json**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_json = spark.read.option(\"multiline\", True).json(\"data/03.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- address: struct (nullable = true)\n",
      " |    |-- city: string (nullable = true)\n",
      " |    |-- state: string (nullable = true)\n",
      " |    |-- zip: long (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- children: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- hobbies: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- married: boolean (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_json.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **CSV**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. All columns are in string format\n",
    "1. Automatic inference of columns types\n",
    "1. Custom columns types\n",
    "1. Read CSV files from a specific directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All columns are in string format\n",
    "df_csv_string = spark.read.csv(\"data/03.csv\", header=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatic inference of columns types\n",
    "df_csv_auto = spark.read.csv(\"data/03.csv\", header=True, inferSchema=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- A: double (nullable = true)\n",
      " |-- B: string (nullable = true)\n",
      " |-- C: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Custom columns types\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "custom_types = StructType(\n",
    "\n",
    "    [\n",
    "        StructField(name='A', dataType=DoubleType(), nullable=True),\n",
    "        StructField(name='B', dataType=StringType(), nullable=True),\n",
    "        StructField(name='C', dataType=IntegerType(), nullable=True)\n",
    "    ]\n",
    "\n",
    ")\n",
    "\n",
    "df_csv_custom = spark.read.csv(\"data/03.csv\", header=True, schema=custom_types)\n",
    "df_csv_custom.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- A: string (nullable = true)\n",
      " |-- B: string (nullable = true)\n",
      " |-- C: string (nullable = true)\n",
      "\n",
      "+---+---+---+\n",
      "|  A|  B|  C|\n",
      "+---+---+---+\n",
      "|1.0|  2|  3|\n",
      "|4.0|  5|  6|\n",
      "|7.0|  8|  9|\n",
      "|2.0|  2|  2|\n",
      "|2.0|  2|  2|\n",
      "|2.0|  2|  2|\n",
      "+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read CSV files from a specific directory\n",
    "df_csv_folder = spark.read.csv(\"data/03-many-csv\", header=True)\n",
    "df_csv_folder.printSchema()\n",
    "df_csv_folder.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Pandas**\n",
    "\n",
    "!!! note \"Pandas 的特色[^pandas-1]\"\n",
    "    1. Scalability beyond a **single** machine  \n",
    "    1. Interactive data visualization  \n",
    "    1. Leveraging unified analytics functionality in Spark  \n",
    "\n",
    "[^pandas-1]:\n",
    "    [Pandas API on Upcoming Apache Spark™ 3.2](https://www.databricks.com/blog/2021/10/04/pandas-api-on-upcoming-apache-spark-3-2.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/pyspark/pandas/__init__.py:49: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pyspark import pandas as ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_csv`, the default index is attached which can cause additional overhead.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.0</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.0</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     A  B  C\n",
       "0  1.0  2  3\n",
       "1  4.0  5  6\n",
       "2  7.0  8  9"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# From csv\n",
    "df_with_spark_pandas = ps.read_csv(\"data/03.csv\")\n",
    "df_with_spark_pandas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Parquet**\n",
    "\n",
    "TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Transform**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_csv_custom.rdd # Transform into rdd format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 16.0, 49.0]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_map = df.map(lambda row : row[\"A\"]**2)\n",
    "demo_map.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(A=7.0, B='8', C=9)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_filter = df.filter(lambda row : row[\"A\"] > 5)\n",
    "demo_filter.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 轉換 | 範例 |\n",
    "| --- | --- |\n",
    "| map() | 將函數應用於RDD中的每個元素，並返回一個新的RDD。例如，rdd.map(lambda x: x * 2)會將RDD中的每個元素乘以2。 |\n",
    "| flatMap() | 將函數應用於RDD中的每個元素，並返回一個新的RDD，其中包含函數返回的所有元素。例如，rdd.flatMap(lambda x: x.split(\" \"))會將RDD中的每個字符串按空格分割，並返回一個包含所有單詞的RDD。 |\n",
    "| filter() | 根據函數的返回值過濾RDD中的元素，並返回一個新的RDD。例如，rdd.filter(lambda x: x % 2 == 0)會過濾掉RDD中的奇數元素。 |\n",
    "| groupByKey() | 根據鍵將RDD中的元素分組，並返回一個新的RDD，其中每個鍵對應一個可迭代的值序列。例如，rdd.groupByKey()會將RDD中的鍵值對按鍵分組。 |\n",
    "| reduceByKey() | 根據鍵將RDD中的元素分組，並使用函數對每組值進行聚合，並返回一個新的RDD，其中每個鍵對應一個聚合後的值。例如，rdd.reduceByKey(lambda x, y: x + y)會將RDD中的鍵值對按鍵分組，並對每組值求和。 |\n",
    "| sortByKey() | 根據鍵將RDD中的元素排序，並返回一個新的RDD。例如，rdd.sortByKey()會將RDD中的鍵值對按鍵升序排序。 |\n",
    "| union() | 將兩個或多個RDD合併為一個新的RDD。例如，rdd1.union(rdd2)會將rdd1和rdd2合併為一個新的RDD。 |\n",
    "| intersection() | 返回兩個或多個RDD共有的元素組成的新的RDD。例如，rdd1.intersection(rdd2)會返回rdd1和rdd2共有的元素組成的新的RDD。 |\n",
    "| distinct() | 返回去除重複元素後的新的RDD。例如，rdd.distinct()會返回去除重複元素後的新的RDD。 |\n",
    "| join() | 根據鍵將兩個或多個RDD中的元素連接起來，並返回一個新的RDD，其中每個鍵對應一個由兩個或多個值組成的元組。例如，rdd1.join(rdd2)會根據鍵將rdd1和rdd2中的元素連接起來，並返回一個新的RDD。 |\n",
    "| leftOuterJoin() | 根據鍵將兩個或多個RDD中的元素連接起來，並返回一個新的RDD，其中每個鍵對應一個由左邊值和右邊值（如果存在"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
